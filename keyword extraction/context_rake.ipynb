{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "516186bd0ec0d9f4d5315d95f88038d756540a3a2a2ced092e94dc29fedad10e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Context and emotions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Which libraries I have used and why\n",
    "\n",
    "### NLTK n-grams \n",
    "N-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places. When performing machine learning tasks related to natural language processing, we usually need to generate n-grams from input sentences. For example, in text classification tasks, in addition to using each individual token found in the corpus, we may want to add bi-grams or tri-grams as features to represent our documents.\n",
    "\n",
    "### TextBlob\n",
    "TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "\n",
    "### NRCLex \n",
    "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing.\n",
    "\n",
    "### Rake\n",
    "RAKE short for Rapid Automatic Keyword Extraction algorithm, is a domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurance with other words in the text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install NRCLex\n",
    "# pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from gensim.parsing.preprocessing import STOPWORDS, strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short, stem_text\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "#pip install NRCLex\n",
    "from nrclex import NRCLex\n",
    "from rake_nltk import Rake\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\deniz\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from provided blog\n",
    "data = \"Getting back to work made a difference. After 10 days of nothing but the business of moving and all of its seemingly obligatory messy emotions, it was nice to think of nothing but my patients. I worked Wednesday through Friday, and even with a couple of long days in there, it was a relief to be away from home. It was a relief to be away from unpacking, and contemplating, and deciding. It was a pleasure to think about somebody other than myself for 3 days. I needed that. Those 3 days away, combined with a long run/walk/dip into Lake Superior with Jet yesterday, gave me the energy to unpack nearly my entire basement today. I ve still got a lot to do, but things are starting to take shape. My bedroom is almost completely put together. My bathroom and kitchen are done. I ve still got boxes in the living room, dining room and the other 2 bedrooms, but I m getting there. Tomorrow I m heading south to Mayo Clinic for a ketamine infusion. Im pleased its not an urgent need at this time, just a regular maintenance dose. Returning to work, getting some exercise, and progressing with my unpacking have each helped stabilize my mood. Im  no longer daily wiping tears from my eyes. In fact, I havent cried for several days. That, in and of itself, is quite a feat! I m taking my time with unpacking. I m doing my best to remain patient. Taking the next right action and maintaining my attitude of gratitude are my focus now. Its still hard, but its not impossible. Settling into my new home, new routine, and new city will take time. I m keeping that fact forefront in my mind. I can do this. But I cant do it all today, nor do I have to. Patiently, Ill get it done.\""
   ]
  },
  {
   "source": [
    "# TextBlob"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['obligatory messy emotions', 'long days', 'long run/walk/dip', 'jet', 'entire basement', 'tomorrow', 'mayo clinic', 'ketamine infusion', 'im', 'urgent need', 'regular maintenance dose', 'im', 'right action', 'settling', 'new home', 'new city', 'fact forefront', 'patiently', 'ill']\n"
     ]
    }
   ],
   "source": [
    "# use blob to extract easy nouns\n",
    "blob = TextBlob(data)\n",
    "print(blob.noun_phrases)\n",
    "nouns_of_data = blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nouns for ngrams\n",
    "# nouns_of_data = ['obligatory messy emotions', 'long days', 'long run/walk/dip', 'jet', 'entire basement', 'tomorrow', 'mayo clinic', 'ketamine infusion', 'im', 'urgent need', 'regular maintenance dose', 'im', 'haven t', 'right action', 'settling', 'new home', 'new city', 'fact forefront', 'patiently', 'ill']\n",
    "# nouns_of_data=str(s)"
   ]
  },
  {
   "source": [
    "# N_grams"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(s, n):\n",
    "    '''returns n_grams\n",
    "    s = data str\n",
    "    n = number of grams'''\n",
    "\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    output = list(ngrams(tokens, n))[1:10]\n",
    "    return output;"
   ]
  },
  {
   "source": [
    "# NRCLex"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate text object (for best results, 'text' should be unicode).\n",
    "text_object = NRCLex(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "314\n['Getting', 'back', 'to', 'work', 'made', 'a', 'difference', 'After', '10', 'days', 'of', 'nothing', 'but', 'the', 'business', 'of', 'moving', 'and', 'all', 'of', 'its', 'seemingly', 'obligatory', 'messy', 'emotions', 'it', 'was', 'nice', 'to', 'think', 'of', 'nothing', 'but', 'my', 'patients', 'I', 'worked', 'Wednesday', 'through', 'Friday', 'and', 'even', 'with', 'a', 'couple', 'of', 'long', 'days', 'in', 'there', 'it', 'was', 'a', 'relief', 'to', 'be', 'away', 'from', 'home', 'It', 'was', 'a', 'relief', 'to', 'be', 'away', 'from', 'unpacking', 'and', 'contemplating', 'and', 'deciding', 'It', 'was', 'a', 'pleasure', 'to', 'think', 'about', 'somebody', 'other', 'than', 'myself', 'for', '3', 'days', 'I', 'needed', 'that', 'Those', '3', 'days', 'away', 'combined', 'with', 'a', 'long', 'run/walk/dip', 'into', 'Lake', 'Superior', 'with', 'Jet', 'yesterday', 'gave', 'me', 'the', 'energy', 'to', 'unpack', 'nearly', 'my', 'entire', 'basement', 'today', 'I', 've', 'still', 'got', 'a', 'lot', 'to', 'do', 'but', 'things', 'are', 'starting', 'to', 'take', 'shape', 'My', 'bedroom', 'is', 'almost', 'completely', 'put', 'together', 'My', 'bathroom', 'and', 'kitchen', 'are', 'done', 'I', 've', 'still', 'got', 'boxes', 'in', 'the', 'living', 'room', 'dining', 'room', 'and', 'the', 'other', '2', 'bedrooms', 'but', 'I', 'm', 'getting', 'there', 'Tomorrow', 'I', 'm', 'heading', 'south', 'to', 'Mayo', 'Clinic', 'for', 'a', 'ketamine', 'infusion', 'Im', 'pleased', 'its', 'not', 'an', 'urgent', 'need', 'at', 'this', 'time', 'just', 'a', 'regular', 'maintenance', 'dose', 'Returning', 'to', 'work', 'getting', 'some', 'exercise', 'and', 'progressing', 'with', 'my', 'unpacking', 'have', 'each', 'helped', 'stabilize', 'my', 'mood', 'Im', 'no', 'longer', 'daily', 'wiping', 'tears', 'from', 'my', 'eyes', 'In', 'fact', 'I', 'havent', 'cried', 'for', 'several', 'days', 'That', 'in', 'and', 'of', 'itself', 'is', 'quite', 'a', 'feat', 'I', 'm', 'taking', 'my', 'time', 'with', 'unpacking', 'I', 'm', 'doing', 'my', 'best', 'to', 'remain', 'patient', 'Taking', 'the', 'next', 'right', 'action', 'and', 'maintaining', 'my', 'attitude', 'of', 'gratitude', 'are', 'my', 'focus', 'now', 'Its', 'still', 'hard', 'but', 'its', 'not', 'impossible', 'Settling', 'into', 'my', 'new', 'home', 'new', 'routine', 'and', 'new', 'city', 'will', 'take', 'time', 'I', 'm', 'keeping', 'that', 'fact', 'forefront', 'in', 'my', 'mind', 'I', 'can', 'do', 'this', 'But', 'I', 'cant', 'do', 'it', 'all', 'today', 'nor', 'do', 'I', 'have', 'to', 'Patiently', 'Ill', 'get', 'it', 'done']\n"
     ]
    }
   ],
   "source": [
    "#Return words list.\n",
    "print(len(text_object.words))\n",
    "print(text_object.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Sentence(\"Getting back to work made a difference.\"),\n",
       " Sentence(\"After 10 days of nothing but the business of moving and all of its seemingly obligatory messy emotions, it was nice to think of nothing but my patients.\"),\n",
       " Sentence(\"I worked Wednesday through Friday, and even with a couple of long days in there, it was a relief to be away from home.\"),\n",
       " Sentence(\"It was a relief to be away from unpacking, and contemplating, and deciding.\"),\n",
       " Sentence(\"It was a pleasure to think about somebody other than myself for 3 days.\"),\n",
       " Sentence(\"I needed that.\"),\n",
       " Sentence(\"Those 3 days away, combined with a long run/walk/dip into Lake Superior with Jet yesterday, gave me the energy to unpack nearly my entire basement today.\"),\n",
       " Sentence(\"I ve still got a lot to do, but things are starting to take shape.\"),\n",
       " Sentence(\"My bedroom is almost completely put together.\"),\n",
       " Sentence(\"My bathroom and kitchen are done.\"),\n",
       " Sentence(\"I ve still got boxes in the living room, dining room and the other 2 bedrooms, but I m getting there.\"),\n",
       " Sentence(\"Tomorrow I m heading south to Mayo Clinic for a ketamine infusion.\"),\n",
       " Sentence(\"Im pleased its not an urgent need at this time, just a regular maintenance dose.\"),\n",
       " Sentence(\"Returning to work, getting some exercise, and progressing with my unpacking have each helped stabilize my mood.\"),\n",
       " Sentence(\"Im  no longer daily wiping tears from my eyes.\"),\n",
       " Sentence(\"In fact, I havent cried for several days.\"),\n",
       " Sentence(\"That, in and of itself, is quite a feat!\"),\n",
       " Sentence(\"I m taking my time with unpacking.\"),\n",
       " Sentence(\"I m doing my best to remain patient.\"),\n",
       " Sentence(\"Taking the next right action and maintaining my attitude of gratitude are my focus now.\"),\n",
       " Sentence(\"Its still hard, but its not impossible.\"),\n",
       " Sentence(\"Settling into my new home, new routine, and new city will take time.\"),\n",
       " Sentence(\"I m keeping that fact forefront in my mind.\"),\n",
       " Sentence(\"I can do this.\"),\n",
       " Sentence(\"But I cant do it all today, nor do I have to.\"),\n",
       " Sentence(\"Patiently, Ill get it done.\")]"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "#Return sentences list.\n",
    "# 26 sentences\n",
    "print(len(text_object.sentences))\n",
    "text_object.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "35\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['disgust',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'joy',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'fear',\n",
       " 'negative',\n",
       " 'surprise',\n",
       " 'anticipation',\n",
       " 'trust',\n",
       " 'anticipation',\n",
       " 'trust',\n",
       " 'anticipation',\n",
       " 'joy',\n",
       " 'positive',\n",
       " 'surprise',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'joy',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'sadness',\n",
       " 'positive',\n",
       " 'trust',\n",
       " 'anticipation',\n",
       " 'trust']"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "#Return affect list.\n",
    "# 35 \"emotions\"\n",
    "print(len(text_object.affect_list))\n",
    "text_object.affect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'messy': ['disgust', 'negative'],\n",
       " 'long': ['anticipation'],\n",
       " 'relief': ['positive'],\n",
       " 'shape': ['positive'],\n",
       " 'completely': ['positive'],\n",
       " 'pleased': ['joy', 'positive'],\n",
       " 'urgent': ['anticipation', 'fear', 'negative', 'surprise'],\n",
       " 'time': ['anticipation'],\n",
       " 'maintenance': ['trust'],\n",
       " 'daily': ['anticipation'],\n",
       " 'fact': ['trust'],\n",
       " 'feat': ['anticipation', 'joy', 'positive', 'surprise'],\n",
       " 'patient': ['anticipation', 'positive'],\n",
       " 'action': ['positive'],\n",
       " 'gratitude': ['joy', 'positive'],\n",
       " 'focus': ['positive'],\n",
       " 'impossible': ['negative', 'sadness'],\n",
       " 'routine': ['positive', 'trust']}"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "#Return affect dictionary.\n",
    "# words that contains emotions and have been found by nrclex\n",
    "print(len(text_object.affect_dict))\n",
    "emo=text_object.affect_dict\n",
    "emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('positive', 0.3142857142857143)]"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "#Return highest emotions.\n",
    "text_object.top_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive words\n",
    "pos = ['relief',\n",
    " 'shape',\n",
    " 'completely',\n",
    " 'pleased',\n",
    " 'feat',\n",
    " 'patient',\n",
    " 'action',\n",
    " 'gratitude',\n",
    " 'focus',\n",
    " 'routine']\n",
    "\n",
    "# sentences that were classified as positive\n",
    "pos_sen = str(\"I worked Wednesday through Friday, and even with a couple of long days in there, it was a relief to be away from home. It was a relief to be away from unpacking, and contemplating, and deciding. I ve still got a lot to do, but things are starting to take shape.My bedroom is almost completely put together. Im pleased its not an urgent need at this time, just a regular maintenance dose. That, in and of itself, is quite a feat!. I m doing my best to remain patient. Taking the next right action and maintaining my attitude of gratitude are my focus now. Settling into my new home, new routine, and new city will take time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['long days', 'shape.my bedroom', 'im', 'urgent need', 'regular maintenance dose', 'right action', 'settling', 'new home', 'new city']\n"
     ]
    }
   ],
   "source": [
    "# use blob to extract easy nouns\n",
    "blob = TextBlob(pos_sen)\n",
    "print(blob.noun_phrases)\n",
    "nouns_of_data = blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('worked', 'wednesday'),\n",
       " ('wednesday', 'through'),\n",
       " ('through', 'friday'),\n",
       " ('friday', 'and'),\n",
       " ('and', 'even'),\n",
       " ('even', 'with'),\n",
       " ('with', 'a'),\n",
       " ('a', 'couple'),\n",
       " ('couple', 'of')]"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "n_grams(s=pos_sen, n=2)"
   ]
  },
  {
   "source": [
    "# RAKE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "r = Rake(min_length=2, max_length=4)\n",
    "\n",
    "r.extract_keywords_from_text(data)\n",
    "\n",
    "# To get keyword phrases ranked highest to lowest.\n",
    "rake_words = str(r.get_ranked_phrases()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"['seemingly obligatory messy emotions', 'longer daily wiping tears', 'almost completely put together', 'regular maintenance dose', 'next right action', 'entire basement today', 'still got boxes', '3 days away', 'still got', '3 days', 'still hard', 'several days', 'long days', '10 days', 'worked wednesday', 'work made', 'urgent need', 'unpack nearly', 'take time', 'take shape', 'remain patient', 'new routine', 'new home', 'new city', 'mayo clinic', 'long run', 'living room', 'lake superior', 'ketamine infusion', 'jet yesterday', 'im pleased', 'ill get', 'helped stabilize', 'heading south', 'havent cried', 'getting back', 'fact forefront', 'dining room', '2 bedrooms']\""
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "rake_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# type of rake\n",
    "type(rake_words)"
   ]
  },
  {
   "source": [
    "# NRCLex"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate text object (for best results, 'text' should be unicode).\n",
    "text_object = NRCLex(rake_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "89\n[\"'seemingly\", 'obligatory', 'messy', 'emotions', \"'longer\", 'daily', 'wiping', 'tears', \"'almost\", 'completely', 'put', 'together', \"'regular\", 'maintenance', 'dose', \"'next\", 'right', 'action', \"'entire\", 'basement', 'today', \"'still\", 'got', 'boxes', '3', 'days', 'away', \"'still\", 'got', '3', 'days', \"'still\", 'hard', \"'several\", 'days', \"'long\", 'days', \"'10\", 'days', \"'worked\", 'wednesday', \"'work\", 'made', \"'urgent\", 'need', \"'unpack\", 'nearly', \"'take\", 'time', \"'take\", 'shape', \"'remain\", 'patient', \"'new\", 'routine', \"'new\", 'home', \"'new\", 'city', \"'mayo\", 'clinic', \"'long\", 'run', \"'living\", 'room', \"'lake\", 'superior', \"'ketamine\", 'infusion', \"'jet\", 'yesterday', \"'im\", 'pleased', \"'ill\", 'get', \"'helped\", 'stabilize', \"'heading\", 'south', \"'havent\", 'cried', \"'getting\", 'back', \"'fact\", 'forefront', \"'dining\", 'room', '2', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "#Return words list.\n",
    "print(len(text_object.words))\n",
    "print(text_object.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Sentence(\"['seemingly obligatory messy emotions', 'longer daily wiping tears', 'almost completely put together', 'regular maintenance dose', 'next right action', 'entire basement today', 'still got boxes', '3 days away', 'still got', '3 days', 'still hard', 'several days', 'long days', '10 days', 'worked wednesday', 'work made', 'urgent need', 'unpack nearly', 'take time', 'take shape', 'remain patient', 'new routine', 'new home', 'new city', 'mayo clinic', 'long run', 'living room', 'lake superior', 'ketamine infusion', 'jet yesterday', 'im pleased', 'ill get', 'helped stabilize', 'heading south', 'havent cried', 'getting back', 'fact forefront', 'dining room', '2 bedrooms']\")]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "#Return sentences list.\n",
    "# 26 sentences\n",
    "print(len(text_object.sentences))\n",
    "text_object.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['disgust',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'trust',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'trust',\n",
       " 'positive',\n",
       " 'joy',\n",
       " 'positive']"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "#Return affect list.\n",
    "# 15 \"emotions\"\n",
    "print(len(text_object.affect_list))\n",
    "text_object.affect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'messy': ['disgust', 'negative'],\n",
       " 'daily': ['anticipation'],\n",
       " 'completely': ['positive'],\n",
       " 'maintenance': ['trust'],\n",
       " 'action': ['positive'],\n",
       " 'time': ['anticipation'],\n",
       " 'shape': ['positive'],\n",
       " 'patient': ['anticipation', 'positive'],\n",
       " 'routine': ['positive', 'trust'],\n",
       " 'superior': ['positive'],\n",
       " 'pleased': ['joy', 'positive']}"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "#Return affect dictionary.\n",
    "# words that contains emotions and have been found by nrclex\n",
    "print(len(text_object.affect_dict))\n",
    "emo=text_object.affect_dict\n",
    "emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "type(emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'pleased'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-a76c9bbfceda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mold_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"pleased\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnew_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"im pleased\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0memo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'pleased'"
     ]
    }
   ],
   "source": [
    "# change keys with the Rake context api\n",
    "old_key = \"pleased\"\n",
    "new_key = \"im pleased\"\n",
    "emo[new_key] = emo.pop(old_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'emo' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e2e3c2504546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# context for the emotions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0memo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'emo' is not defined"
     ]
    }
   ],
   "source": [
    "# context for the emotions\n",
    "emo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo = {'seemingly obligatory messy emotions': ['disgust', 'negative'],\n",
    " 'longer daily wiping tears': ['anticipation'],\n",
    " 'almost completely put together': ['positive'],\n",
    " 'regular maintenance dose': ['trust'],\n",
    " 'next right action': ['positive'],\n",
    " 'take time': ['anticipation'],\n",
    " 'take shape': ['positive'],\n",
    " 'remain patient': ['anticipation', 'positive'],\n",
    " 'new routine': ['positive', 'trust'],\n",
    " 'lake superior': ['positive'],\n",
    " 'im pleased': ['joy', 'positive']}"
   ]
  },
  {
   "source": [
    "# Visualisations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'seemingly obligatory messy emotions': ['disgust', 'negative'],\n",
       " 'longer daily wiping tears': ['anticipation'],\n",
       " 'almost completely put together': ['positive'],\n",
       " 'regular maintenance dose': ['trust'],\n",
       " 'next right action': ['positive'],\n",
       " 'take time': ['anticipation'],\n",
       " 'take shape': ['positive'],\n",
       " 'remain patient': ['anticipation', 'positive'],\n",
       " 'new routine': ['positive', 'trust'],\n",
       " 'lake superior': ['positive'],\n",
       " 'im pleased': ['joy', 'positive']}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "type(emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-081490e36044>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"off\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\deniz\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \"\"\"\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\deniz\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \"\"\"\n\u001b[1;32m--> 613\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\deniz\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[0mregexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m         \u001b[1;31m# remove 's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         words = [word[:-2] if word.lower().endswith(\"'s\") else word\n",
      "\u001b[1;32mD:\\deniz\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}