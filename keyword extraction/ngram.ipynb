{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "516186bd0ec0d9f4d5315d95f88038d756540a3a2a2ced092e94dc29fedad10e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "N-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.\n",
    "\n",
    "When performing machine learning tasks related to natural language processing, we usually need to generate n-grams from input sentences. For example, in text classification tasks, in addition to using each individual token found in the corpus, we may want to add bi-grams or tri-grams as features to represent our documents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\deniz\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from gensim.parsing.preprocessing import STOPWORDS, strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short, stem_text\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "#pip install NRCLex\n",
    "from nrclex import NRCLex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data = \"Getting back to work made a difference. After 10 days of nothing but the business of moving and all of its seemingly obligatory messy emotions, it was nice to think of nothing but my patients. I worked Wednesday through Friday, and even with a couple of long days in there, it was a relief to be away from home. It was a relief to be away from unpacking, and contemplating, and deciding. It was a pleasure to think about somebody other than myself for 3 days. I needed that. Those 3 days away, combined with a long run/walk/dip into Lake Superior with Jet yesterday, gave me the energy to unpack nearly my entire basement today. I ve still got a lot to do, but things are starting to take shape. My bedroom is almost completely put together. My bathroom and kitchen are done. I ve still got boxes in the living room, dining room and the other 2 bedrooms, but I m getting there. Tomorrow I m heading south to Mayo Clinic for a ketamine infusion. Im pleased its not an urgent need at this time, just a regular maintenance dose. Returning to work, getting some exercise, and progressing with my unpacking have each helped stabilize my mood. Im  no longer daily wiping tears from my eyes. In fact, I haven t cried for several days. That, in and of itself, is quite a feat! I m taking my time with unpacking. I m doing my best to remain patient. Taking the next right action and maintaining my attitude of gratitude are my focus now. Its still hard, but its not impossible. Settling into my new home, new routine, and new city will take time. I m keeping that fact forefront in my mind. I can do this. But I cant do it all today, nor do I have to. Patiently, Ill get it done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['obligatory messy emotions', 'long days', 'long run/walk/dip', 'jet', 'entire basement', 'tomorrow', 'mayo clinic', 'ketamine infusion', 'im', 'urgent need', 'regular maintenance dose', 'im', 'haven t', 'right action', 'settling', 'new home', 'new city', 'fact forefront', 'patiently', 'ill']\n"
     ]
    }
   ],
   "source": [
    "# use blob to extract easy nouns\n",
    "blob = TextBlob(data)\n",
    "print(blob.noun_phrases)\n",
    "nouns_of_data = blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nouns for ngrams\n",
    "# nouns_of_data = ['obligatory messy emotions', 'long days', 'long run/walk/dip', 'jet', 'entire basement', 'tomorrow', 'mayo clinic', 'ketamine infusion', 'im', 'urgent need', 'regular maintenance dose', 'im', 'haven t', 'right action', 'settling', 'new home', 'new city', 'fact forefront', 'patiently', 'ill']\n",
    "# nouns_of_data=str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(s, n):\n",
    "    '''returns n_grams\n",
    "    s = data str\n",
    "    n = number of grams'''\n",
    "\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    output = list(ngrams(tokens, n))[1:10]\n",
    "    return output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate text object (for best results, 'text' should be unicode).\n",
    "text_object = NRCLex(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "315\n['Getting', 'back', 'to', 'work', 'made', 'a', 'difference', 'After', '10', 'days', 'of', 'nothing', 'but', 'the', 'business', 'of', 'moving', 'and', 'all', 'of', 'its', 'seemingly', 'obligatory', 'messy', 'emotions', 'it', 'was', 'nice', 'to', 'think', 'of', 'nothing', 'but', 'my', 'patients', 'I', 'worked', 'Wednesday', 'through', 'Friday', 'and', 'even', 'with', 'a', 'couple', 'of', 'long', 'days', 'in', 'there', 'it', 'was', 'a', 'relief', 'to', 'be', 'away', 'from', 'home', 'It', 'was', 'a', 'relief', 'to', 'be', 'away', 'from', 'unpacking', 'and', 'contemplating', 'and', 'deciding', 'It', 'was', 'a', 'pleasure', 'to', 'think', 'about', 'somebody', 'other', 'than', 'myself', 'for', '3', 'days', 'I', 'needed', 'that', 'Those', '3', 'days', 'away', 'combined', 'with', 'a', 'long', 'run/walk/dip', 'into', 'Lake', 'Superior', 'with', 'Jet', 'yesterday', 'gave', 'me', 'the', 'energy', 'to', 'unpack', 'nearly', 'my', 'entire', 'basement', 'today', 'I', 've', 'still', 'got', 'a', 'lot', 'to', 'do', 'but', 'things', 'are', 'starting', 'to', 'take', 'shape', 'My', 'bedroom', 'is', 'almost', 'completely', 'put', 'together', 'My', 'bathroom', 'and', 'kitchen', 'are', 'done', 'I', 've', 'still', 'got', 'boxes', 'in', 'the', 'living', 'room', 'dining', 'room', 'and', 'the', 'other', '2', 'bedrooms', 'but', 'I', 'm', 'getting', 'there', 'Tomorrow', 'I', 'm', 'heading', 'south', 'to', 'Mayo', 'Clinic', 'for', 'a', 'ketamine', 'infusion', 'Im', 'pleased', 'its', 'not', 'an', 'urgent', 'need', 'at', 'this', 'time', 'just', 'a', 'regular', 'maintenance', 'dose', 'Returning', 'to', 'work', 'getting', 'some', 'exercise', 'and', 'progressing', 'with', 'my', 'unpacking', 'have', 'each', 'helped', 'stabilize', 'my', 'mood', 'Im', 'no', 'longer', 'daily', 'wiping', 'tears', 'from', 'my', 'eyes', 'In', 'fact', 'I', 'haven', 't', 'cried', 'for', 'several', 'days', 'That', 'in', 'and', 'of', 'itself', 'is', 'quite', 'a', 'feat', 'I', 'm', 'taking', 'my', 'time', 'with', 'unpacking', 'I', 'm', 'doing', 'my', 'best', 'to', 'remain', 'patient', 'Taking', 'the', 'next', 'right', 'action', 'and', 'maintaining', 'my', 'attitude', 'of', 'gratitude', 'are', 'my', 'focus', 'now', 'Its', 'still', 'hard', 'but', 'its', 'not', 'impossible', 'Settling', 'into', 'my', 'new', 'home', 'new', 'routine', 'and', 'new', 'city', 'will', 'take', 'time', 'I', 'm', 'keeping', 'that', 'fact', 'forefront', 'in', 'my', 'mind', 'I', 'can', 'do', 'this', 'But', 'I', 'cant', 'do', 'it', 'all', 'today', 'nor', 'do', 'I', 'have', 'to', 'Patiently', 'Ill', 'get', 'it', 'done']\n"
     ]
    }
   ],
   "source": [
    "#Return words list.\n",
    "print(len(text_object.words))\n",
    "print(text_object.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Sentence(\"Getting back to work made a difference.\"),\n",
       " Sentence(\"After 10 days of nothing but the business of moving and all of its seemingly obligatory messy emotions, it was nice to think of nothing but my patients.\"),\n",
       " Sentence(\"I worked Wednesday through Friday, and even with a couple of long days in there, it was a relief to be away from home.\"),\n",
       " Sentence(\"It was a relief to be away from unpacking, and contemplating, and deciding.\"),\n",
       " Sentence(\"It was a pleasure to think about somebody other than myself for 3 days.\"),\n",
       " Sentence(\"I needed that.\"),\n",
       " Sentence(\"Those 3 days away, combined with a long run/walk/dip into Lake Superior with Jet yesterday, gave me the energy to unpack nearly my entire basement today.\"),\n",
       " Sentence(\"I ve still got a lot to do, but things are starting to take shape.\"),\n",
       " Sentence(\"My bedroom is almost completely put together.\"),\n",
       " Sentence(\"My bathroom and kitchen are done.\"),\n",
       " Sentence(\"I ve still got boxes in the living room, dining room and the other 2 bedrooms, but I m getting there.\"),\n",
       " Sentence(\"Tomorrow I m heading south to Mayo Clinic for a ketamine infusion.\"),\n",
       " Sentence(\"Im pleased its not an urgent need at this time, just a regular maintenance dose.\"),\n",
       " Sentence(\"Returning to work, getting some exercise, and progressing with my unpacking have each helped stabilize my mood.\"),\n",
       " Sentence(\"Im  no longer daily wiping tears from my eyes.\"),\n",
       " Sentence(\"In fact, I haven t cried for several days.\"),\n",
       " Sentence(\"That, in and of itself, is quite a feat!\"),\n",
       " Sentence(\"I m taking my time with unpacking.\"),\n",
       " Sentence(\"I m doing my best to remain patient.\"),\n",
       " Sentence(\"Taking the next right action and maintaining my attitude of gratitude are my focus now.\"),\n",
       " Sentence(\"Its still hard, but its not impossible.\"),\n",
       " Sentence(\"Settling into my new home, new routine, and new city will take time.\"),\n",
       " Sentence(\"I m keeping that fact forefront in my mind.\"),\n",
       " Sentence(\"I can do this.\"),\n",
       " Sentence(\"But I cant do it all today, nor do I have to.\"),\n",
       " Sentence(\"Patiently, Ill get it done.\")]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "#Return sentences list.\n",
    "# 26 sentences\n",
    "print(len(text_object.sentences))\n",
    "text_object.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "37\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['disgust',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'joy',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'fear',\n",
       " 'negative',\n",
       " 'surprise',\n",
       " 'anticipation',\n",
       " 'trust',\n",
       " 'anticipation',\n",
       " 'trust',\n",
       " 'positive',\n",
       " 'trust',\n",
       " 'anticipation',\n",
       " 'joy',\n",
       " 'positive',\n",
       " 'surprise',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'joy',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'sadness',\n",
       " 'positive',\n",
       " 'trust',\n",
       " 'anticipation',\n",
       " 'trust']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#Return affect list.\n",
    "# 37 \"emotions\"\n",
    "print(len(text_object.affect_list))\n",
    "text_object.affect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'messy': ['disgust', 'negative'],\n",
       " 'long': ['anticipation'],\n",
       " 'relief': ['positive'],\n",
       " 'shape': ['positive'],\n",
       " 'completely': ['positive'],\n",
       " 'pleased': ['joy', 'positive'],\n",
       " 'urgent': ['anticipation', 'fear', 'negative', 'surprise'],\n",
       " 'time': ['anticipation'],\n",
       " 'maintenance': ['trust'],\n",
       " 'daily': ['anticipation'],\n",
       " 'fact': ['trust'],\n",
       " 'haven': ['positive', 'trust'],\n",
       " 'feat': ['anticipation', 'joy', 'positive', 'surprise'],\n",
       " 'patient': ['anticipation', 'positive'],\n",
       " 'action': ['positive'],\n",
       " 'gratitude': ['joy', 'positive'],\n",
       " 'focus': ['positive'],\n",
       " 'impossible': ['negative', 'sadness'],\n",
       " 'routine': ['positive', 'trust']}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#Return affect dictionary.\n",
    "# words that contains emotions and have been found by nrclex\n",
    "print(len(text_object.affect_dict))\n",
    "text_object.affect_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'disgust': 1,\n",
       " 'negative': 3,\n",
       " 'anticipation': 9,\n",
       " 'positive': 12,\n",
       " 'joy': 3,\n",
       " 'fear': 1,\n",
       " 'surprise': 2,\n",
       " 'trust': 5,\n",
       " 'sadness': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "#Return raw emotional counts.\n",
    "text_object.raw_emotion_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('positive', 0.32432432432432434)]"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "#Return highest emotions.\n",
    "text_object.top_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'fear': 0.02702702702702703,\n",
       " 'anger': 0.0,\n",
       " 'anticip': 0.0,\n",
       " 'trust': 0.13513513513513514,\n",
       " 'surprise': 0.05405405405405406,\n",
       " 'positive': 0.32432432432432434,\n",
       " 'negative': 0.08108108108108109,\n",
       " 'sadness': 0.02702702702702703,\n",
       " 'disgust': 0.02702702702702703,\n",
       " 'joy': 0.08108108108108109,\n",
       " 'anticipation': 0.24324324324324326}"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "#Return affect frequencies.\n",
    "text_object.affect_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "# check data type\n",
    "type(text_object.affect_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'disgust': 1,\n",
       " 'negative': 3,\n",
       " 'anticipation': 9,\n",
       " 'positive': 12,\n",
       " 'joy': 3,\n",
       " 'fear': 1,\n",
       " 'surprise': 2,\n",
       " 'trust': 5,\n",
       " 'sadness': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "text_object.raw_emotion_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('maintenance', ['trust']), ('fact', ['trust']), ('haven', ['positive', 'trust']), ('routine', ['positive', 'trust']), ('relief', ['positive'])]\n['maintenance', 'fact', 'haven', 'routine', 'relief']\n"
     ]
    }
   ],
   "source": [
    "# get the 3 top emotions\n",
    "from collections import Counter\n",
    "c = Counter(text_object.affect_dict)\n",
    "\n",
    "# return top 3 pairs\n",
    "most_common = c.most_common(5)\n",
    "# For getting the keys from most common\n",
    "my_keys = [key for key, val in most_common]\n",
    "\n",
    "print(most_common)\n",
    "print(my_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "res = str(text_object.affect_dict)\n",
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# check data type \n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = str(['messy',\n",
    " 'long',\n",
    " 'relief',\n",
    " 'shape',\n",
    " 'completely',\n",
    " 'pleased',\n",
    " 'urgent',\n",
    " 'time',\n",
    " 'maintenance',\n",
    " 'daily',\n",
    " 'fact',\n",
    " 'haven',\n",
    " 'feat',\n",
    " 'patient',\n",
    " 'action',\n",
    " 'gratitude',\n",
    " 'focus',\n",
    " 'impossible',\n",
    " 'routine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy = str(text_object.sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['obligatory messy emotions']\n"
     ]
    }
   ],
   "source": [
    "# use nouns for ngrams\n",
    "# use blob to extract easy nouns\n",
    "blob = TextBlob(messy)\n",
    "print(blob.noun_phrases)\n",
    "nouns_of_data = blob.noun_phrases  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'After 10 days of nothing but the business of moving and all of its seemingly obligatory messy emotions, it was nice to think of nothing but my patients.'"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('disgust', 'negative'),\n",
       " ('negative', 'long'),\n",
       " ('long', 'anticipation'),\n",
       " ('anticipation', 'relief'),\n",
       " ('relief', 'positive'),\n",
       " ('positive', 'shape'),\n",
       " ('shape', 'positive'),\n",
       " ('positive', 'completely'),\n",
       " ('completely', 'positive')]"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# test bigram\n",
    "n_grams(s=res, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'score_ngram' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-0180896cddd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore_ngram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'score_ngram' is not defined"
     ]
    }
   ],
   "source": [
    "score_ngram(messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}